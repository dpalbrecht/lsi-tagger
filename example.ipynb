{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6845d714-1fe0-43b0-8b7d-9ab583b3d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "sys.path.append(\"c:/users/david/desktop/sandbox/lsi-tagger\")\n",
    "from model.tag_extractor import TagExtractor\n",
    "from model.text_cleaner import TextCleaner\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b076de-bf48-4685-b0c3-34ca8f9fc35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105126, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>product_code</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_no</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_no</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_code</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>...</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_code</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_no</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_no</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_no</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>10</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  product_code  prod_name  product_type_no product_type_name  \\\n",
       "0   108775015        108775  Strap top              253          Vest top   \n",
       "1   108775044        108775  Strap top              253          Vest top   \n",
       "\n",
       "   product_group_name  graphical_appearance_no graphical_appearance_name  \\\n",
       "0  Garment Upper body                  1010016                     Solid   \n",
       "1  Garment Upper body                  1010016                     Solid   \n",
       "\n",
       "   colour_group_code colour_group_name  ...  department_name index_code  \\\n",
       "0                  9             Black  ...     Jersey Basic          A   \n",
       "1                 10             White  ...     Jersey Basic          A   \n",
       "\n",
       "   index_name index_group_no  index_group_name section_no  \\\n",
       "0  Ladieswear              1        Ladieswear         16   \n",
       "1  Ladieswear              1        Ladieswear         16   \n",
       "\n",
       "             section_name garment_group_no  garment_group_name  \\\n",
       "0  Womens Everyday Basics             1002        Jersey Basic   \n",
       "1  Womens Everyday Basics             1002        Jersey Basic   \n",
       "\n",
       "                               detail_desc  \n",
       "0  Jersey top with narrow shoulder straps.  \n",
       "1  Jersey top with narrow shoulder straps.  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/h&m_kaggle_products.csv')\n",
    "df = df[~df.isnull().any(axis=1)]\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567abae3-35f4-40ea-bbaf-67508e331fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 43404/43404 [00:04<00:00, 9768.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = df['detail_desc'].drop_duplicates().values\n",
    "cleaned_documents = TextCleaner(word_count_min=2, word_length_min=2).clean_documents(documents)\n",
    "len(cleaned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1556f58f-3292-47c6-97ba-c13daf06e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_11928\\2820098043.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cleaned_documents = np.array(cleaned_documents)[doc_keep_inds].tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43395, 43395)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove empty documents\n",
    "doc_keep_inds = np.array([n for n,doc in enumerate(cleaned_documents) if len(doc)>0])\n",
    "cleaned_documents = np.array(cleaned_documents)[doc_keep_inds].tolist()\n",
    "documents = np.array(documents)[doc_keep_inds].tolist()\n",
    "\n",
    "len(documents), len(cleaned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef35d04-1d2c-40ea-8412-93d07003c2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSI (https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html)\n",
    "  # https://radimrehurek.com/gensim/models/lsimodel.html\n",
    "dictionary = corpora.Dictionary(cleaned_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents]\n",
    "\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95eb0b08-a615-475c-81f4-3094cb7d5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420df802-8595-4278-8d99-5cdd5d0ed6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.179*\"long\" + 0.178*\"sleeves\" + 0.168*\"soft\" + 0.166*\"short\" + 0.165*\"cuffs\" + 0.163*\"hem\" + 0.161*\"pockets\" + 0.159*\"cotton\" + 0.153*\"jersey\" + 0.148*\"waist\"'),\n",
       " (1,\n",
       "  '-0.234*\"knit\" + 0.232*\"pockets\" + -0.223*\"ribbing\" + -0.211*\"jumper\" + 0.206*\"fly\" + -0.198*\"neckline\" + -0.183*\"long\" + 0.174*\"legs\" + -0.174*\"hem\" + -0.164*\"cuffs\"'),\n",
       " (2,\n",
       "  '0.233*\"dress\" + 0.205*\"seam\" + -0.202*\"ribbing\" + 0.195*\"neck\" + -0.191*\"pockets\" + 0.179*\"short\" + -0.177*\"sweatshirt\" + 0.165*\"skirt\" + 0.151*\"unlined\" + 0.149*\"weave\"'),\n",
       " (3,\n",
       "  '0.332*\"imitation\" + 0.323*\"leather\" + 0.286*\"insoles\" + 0.286*\"soles\" + 0.256*\"linings\" + 0.245*\"cm\" + 0.220*\"rubber\" + 0.193*\"loop\" + 0.154*\"heel\" + 0.139*\"lacing\"'),\n",
       " (4,\n",
       "  '0.280*\"jersey\" + -0.263*\"collar\" + -0.221*\"buttons\" + 0.206*\"elasticated\" + -0.190*\"buttoned\" + -0.180*\"jacket\" + -0.179*\"chest\" + 0.164*\"soft\" + -0.159*\"flap\" + -0.156*\"cuffs\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e610fd64-d13c-4156-9364-ab8c7c988854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_corpus2vec(lsi_corpus_vec):\n",
    "    vec = np.zeros(200)\n",
    "    for ind, value in lsi_corpus_vec:\n",
    "        vec[ind] = value\n",
    "    return vec\n",
    "\n",
    "lsi_vecs = np.array([lsi_corpus2vec(cl) for cl in corpus_lsi])\n",
    "lsi_vecs_normed = lsi_vecs/np.linalg.norm(lsi_vecs, axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b58e5f3-27e0-4e98-ac0f-3921c5dcb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_topic_matrix = lsi_model.get_topics()\n",
    "\n",
    "def extract_tags(input_ind, candidate_ind, n_tags=5):\n",
    "\n",
    "    # Get shared words using the sparse TF-IDF corpus\n",
    "    shared_word_ids = np.array(list(set(np.array(corpus_tfidf[input_ind])[:,0]) \\\n",
    "                                  & set(np.array(corpus_tfidf[candidate_ind])[:,0]))).astype(int)\n",
    "\n",
    "    # Get word-topic vectors for those shared words\n",
    "    shared_word_topics = lsi_topic_matrix.T[shared_word_ids]\n",
    "\n",
    "    # Sum those word-topic vectors weighed by each document's topic loadings\n",
    "    shared_lsi_word_loadings = np.sum((shared_word_topics*np.array(corpus_lsi[input_ind])[:,1]) \\\n",
    "                                    + (shared_word_topics*np.array(corpus_lsi[candidate_ind])[:,1]), axis=1)\n",
    "\n",
    "    # Rank shared words by highest weighed word-topic loadings\n",
    "    ranking = np.argsort(-shared_lsi_word_loadings)\n",
    "    ranked_shared_word_ids = shared_word_ids[ranking]\n",
    "\n",
    "    tags = [dictionary[r] for r in ranked_shared_word_ids][:n_tags]\n",
    "    scores = shared_lsi_word_loadings[ranking][:n_tags]\n",
    "    return tags, scores\n",
    "\n",
    "def show_recommendations_with_tags(input_ind, internal_n_recs=50, show_n_recs=5,\n",
    "                                   n_input_tags=10, n_candidate_tags=5):\n",
    "    sims = lsi_vecs_normed[input_ind].dot(lsi_vecs_normed.T)\n",
    "    top_inds = np.argsort(-sims)[:internal_n_recs]\n",
    "\n",
    "    results = []\n",
    "    all_tags = []\n",
    "    for candidate_ind in top_inds:\n",
    "        tags, scores = extract_tags(input_ind, candidate_ind, n_tags=n_candidate_tags)\n",
    "        results.append([candidate_ind, tags, scores])\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "    print(documents[input_ind])\n",
    "    print(Counter(all_tags).most_common(n_input_tags))\n",
    "    print('='*150)\n",
    "    print()\n",
    "\n",
    "    for data in results[:show_n_recs]:\n",
    "        print(documents[data[0]])\n",
    "        display(list(zip(data[1], data[2])))\n",
    "        print('-'*150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51457f16-22ee-48f1-a214-988220e72078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh hi-tops with rubber details, a rib-knit shaft with a wide elastic strap over the foot, and a loop at the back. Mesh insoles and rubber soles.\n",
      "[('mesh', 50), ('soles', 49), ('loop', 46), ('insoles', 33), ('rubber', 32), ('tops', 31), ('elastic', 4), ('rib', 2), ('hi', 2), ('strap', 1)]\n",
      "======================================================================================================================================================\n",
      "\n",
      "Mesh hi-tops with rubber details, a rib-knit shaft with a wide elastic strap over the foot, and a loop at the back. Mesh insoles and rubber soles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mesh', 0.7141216636722489),\n",
       " ('tops', 0.5836421685610182),\n",
       " ('rib', 0.4333607306618575),\n",
       " ('soles', 0.3941848233532636),\n",
       " ('rubber', 0.374554817370311)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Mesh hi-tops with rubber details, and a rib-knit shaft with a loop at the back and lacing at the front. Mesh linings and insoles and chunky rubber soles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mesh', 0.733082668674957),\n",
       " ('tops', 0.5844671799428358),\n",
       " ('rib', 0.434034138792625),\n",
       " ('soles', 0.41102535058882755),\n",
       " ('loop', 0.39970960714371945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Scuba fabric hi-tops with woven sections, reflective details, a zip on each side and a loop at the back. Mesh linings and insoles and rubber soles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tops', 0.6143677620186443),\n",
       " ('mesh', 0.5968893830141513),\n",
       " ('loop', 0.39843057420224576),\n",
       " ('soles', 0.37226304386257814),\n",
       " ('rubber', 0.36144842336369437)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hi-tops in mesh and scuba fabric with lacing at the front and loops front and back. EVA (ethylene-vinyl acetate) and rubber soles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mesh', 0.5541892809320705),\n",
       " ('tops', 0.5350643417458049),\n",
       " ('soles', 0.3085499682650582),\n",
       " ('rubber', 0.29176841990369773),\n",
       " ('hi', 0.150032595401553)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Hi-tops in imitation leather with a scuba shaft, loop front and back, mesh insoles and rubber soles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tops', 0.6446047523745317),\n",
       " ('mesh', 0.5784170302062039),\n",
       " ('loop', 0.41264903088190463),\n",
       " ('soles', 0.3687503241957863),\n",
       " ('rubber', 0.33900985231446545)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ability to re-rank by selecting tags\n",
    "show_recommendations_with_tags(10000, internal_n_recs=50, show_n_recs=5,\n",
    "                               n_input_tags=10, n_candidate_tags=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb6fd2-5372-4475-b307-079692aa0461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a428a9-5565-46e9-8643-85597d3f963f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ea9af-20bf-4c74-aa4d-8bc137968845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagExtractor:\n",
    "    def __init__(self, \n",
    "                 word_count_min=2, \n",
    "                 word_length_min=2, \n",
    "                 num_lsi_topics=300):\n",
    "        self.word_count_min = word_count_min\n",
    "        self.word_length_min = word_length_min\n",
    "        self.num_lsi_topics = num_lsi_topics\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        # Clean text\n",
    "        # TODO: TextCleaner needs a fit and transform method so that I can look up word counts for new documents\n",
    "        self.tc = TextCleaner(word_count_min=self.word_count_min, \n",
    "                              word_length_min=self.word_length_min)\n",
    "        cleaned_documents = self.tc.clean_documents(documents)\n",
    "            \n",
    "        # Create document lookup\n",
    "        self.problem_docs = []\n",
    "        self.doc2ind = {}\n",
    "        for n, doc, cleaned_doc in enumerate(zip(documents, cleaned_documents)):\n",
    "            if len(cleaned_doc)==0:\n",
    "                self.problem_docs.append(doc)\n",
    "            doc2ind[doc] = n\n",
    "                \n",
    "        # Warn for empty documents\n",
    "        if len(self.problem_docs) > 0:\n",
    "            print(\"\"\"Warning: Some documents yield no clean tokens. \n",
    "            These documents won't have tags.\n",
    "            Check self.problem_docs for more detail.\"\"\")\n",
    "        \n",
    "        # Train TF-IDF\n",
    "        self.dictionary = corpora.Dictionary(cleaned_documents)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in cleaned_documents]\n",
    "        self.tfidf = models.TfidfModel(self.corpus)\n",
    "        self.corpus_tfidf = self.tfidf[self.corpus]\n",
    "\n",
    "        # Train LSI\n",
    "        self.lsi_model = models.LsiModel(self.corpus_tfidf, \n",
    "                                         id2word=self.dictionary, \n",
    "                                         num_topics=self.num_lsi_topics)\n",
    "        self.corpus_lsi = self.lsi_model[self.corpus_tfidf]\n",
    "        \n",
    "        # Save the topic matrix for tag extraction\n",
    "        self.lsi_topic_matrix = self.lsi_model.get_topics()\n",
    "        \n",
    "    def _transform(self, document):\n",
    "        doc_ind = doc2ind.get(document) \n",
    "        if doc_ind is None:\n",
    "            # TODO: TextCleaner needs a fit and transform method so that I can look up word counts for new documents\n",
    "            cleaned_documents = self.tc.clean_documents([document])\n",
    "            corpus = self.dictionary.doc2bow(cleaned_documents[0])\n",
    "            corpus_tfidf = self.tfidf[corpus]\n",
    "            corpus_lsi = self.lsi_model[corpus_tfidf] # TODO: Can I just add this to self.corpus_lsi so it gets added online? Same for TF-IDF\n",
    "        else:\n",
    "            corpus_tfidf = self.corpus_tfidf[doc_ind]\n",
    "            corpus_lsi = self.corpus_lsi[doc_ind]\n",
    "        return corpus_tfidf, corpus_lsi\n",
    "        \n",
    "    def extract_tags(self, document_a, document_b, n_tags):\n",
    "        corpus_tfidf_a, corpus_lsi_a = self.transform(document_a)\n",
    "        corpus_tfidf_b, corpus_lsi_b = self.transform(document_b)\n",
    "        \n",
    "        # Get shared words using the sparse TF-IDF corpus\n",
    "        shared_word_ids = np.array(list(set(np.array(corpus_tfidf_a)[:,0]) \\\n",
    "                                      & set(np.array(corpus_tfidf_b)[:,0]))).astype(int)\n",
    "\n",
    "        # Get word-topic vectors for those shared words\n",
    "        shared_word_topics = self.lsi_topic_matrix.T[shared_word_ids]\n",
    "\n",
    "        # Sum those word-topic vectors weighed by each document's topic loadings\n",
    "        shared_lsi_word_loadings = np.sum((shared_word_topics*np.array(corpus_lsi_a)[:,1]) \\\n",
    "                                        + (shared_word_topics*np.array(corpus_lsi_b)[:,1]), axis=1)\n",
    "\n",
    "        # Rank shared words by highest weighed word-topic loadings\n",
    "        ranking = np.argsort(-shared_lsi_word_loadings)\n",
    "        ranked_shared_word_ids = shared_word_ids[ranking]\n",
    "\n",
    "        tags = [self.dictionary[r] for r in ranked_shared_word_ids][:n_tags]\n",
    "        scores = shared_lsi_word_loadings[ranking][:n_tags]\n",
    "        return tags, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbe9e9-9374-4e87-9fd6-a30d62764902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
